{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from pypdf import PdfReader\n",
    "import matplotlib.pyplot as plt\n",
    "import ollama\n",
    "import spacy\n",
    "import dill as pickle\n",
    "import pymupdf\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "from docx import Document\n",
    "from docx.shared import Inches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model for ollama\n",
    "#MODEL = \"mistral\" \n",
    "MODEL = \"llama3.1\"\n",
    "#MODEL = \"qwen:14b\"\n",
    "#MODEL = \"phi4\"\n",
    "\n",
    "SPACY_MODEL = \"en_core_web_sm\"\n",
    "\n",
    "# Set folder paths\n",
    "PDF_FOLDER = \"Codings\"\n",
    "OUTPUT_FOLDER = os.path.join(\"Processed\", MODEL)\n",
    "PROCESSED_DOC_FILENAME = f\"{datetime.now().strftime(\"%y%m%d\")}-{MODEL}-processed_documents.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaHandler:\n",
    "    def __init__(self, model_name: str = MODEL):\n",
    "        self.model = model_name\n",
    "        \n",
    "    def generate_short_summary(self, text: str) -> str:\n",
    "        prompt = f\"Summarize the following text in exactly one (!) sentence withouth any further comments. Start your answer with 'The article is about...'. Article: {text}\"\n",
    "        response = ollama.generate(model=self.model, prompt=prompt)\n",
    "        original_response = response[\"response\"]\n",
    "        \n",
    "        return original_response.split(\".\")[0] + \".\"\n",
    "    \n",
    "    def generate_summary(self, text: str) -> str:\n",
    "        prompt = f\"Your are a research assistant and have been asked to summarize the following text in exactly 4 bullet points. It is extremely important that it is only four bullet points. Avoid adding any further comments or information that appears not in the text. Avoid mentioning that you are a research assistant and what your task is in your response. Here is the text you need to summarize: {text}\"\n",
    "        response = ollama.generate(model=self.model, prompt=prompt)\n",
    "        return response[\"response\"]\n",
    "    \n",
    "    def answer_question(self, text: str, question: str) -> str:\n",
    "        prompt = f\"Answer the following question based on the text, be concise and only mention topics that occured in the text: {text} Question: {question}\"\n",
    "        response = ollama.generate(model=self.model, prompt=prompt)\n",
    "        return response[\"response\"]\n",
    "    \n",
    "    def analyze_sentiment(self, text: str) -> float:\n",
    "        prompt = f\"Analyze the sentiment of the following text and respond with a single number between -5 (very negative towards AI, mentioned mainly risks) and +5 (very positive towards AI, mentioned mainly opportunities) without any comments: {text}\"\n",
    "        response = ollama.generate(model=self.model, prompt=prompt, system=\"You\")\n",
    "        try:\n",
    "            sentiment = float(re.search(r\"-?\\d+\\.?\\d*\", response[\"response\"]).group())\n",
    "            return max(min(sentiment, 5), -5)\n",
    "        except:\n",
    "            return 0.0\n",
    "        \n",
    "    def extract_entities(self, text: str) -> list:\n",
    "        prompt = f\"Extract the entities (Actors/spokespeople/institutions) from the following text. Separate them by a semicolon ; and only answer with the entities without any other comments: {text}\"\n",
    "        response = ollama.generate(model=self.model, prompt=prompt)\n",
    "        \n",
    "        response = response[\"response\"].replace(\"\\n\", \"\")\n",
    "                   \n",
    "        return response.split(\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PdfDocument:\n",
    "    def __init__(self, path: str, content: str, title: str):\n",
    "        self.path = path\n",
    "        self.content = content\n",
    "        self.title = title\n",
    "        self.filename = self.path.split(\"/\")[-1].split(\".\")[0]\n",
    "        self.short_summary = \"\"\n",
    "        self.summary = \"\"\n",
    "        self.sentiment = 0.0\n",
    "        self.entities = []\n",
    "        self.highlighted_sentences = defaultdict(list)     # Initialize dictionary to store sentences by highlight color\n",
    "        self.wordcloud_data = [] # List to store the information for the wordclouds\n",
    "        self.questions = []\n",
    "        \n",
    "        print(f\"{datetime.now().strftime(\"%H:%M:%S\")} Initialized PdfDocument: <{self.title}>\")\n",
    "        \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            \"path\": self.path,\n",
    "            \"content\": self.content,\n",
    "            \"title\": self.title,\n",
    "            \"short_summary\": self.short_summary,\n",
    "            \"summary\": self.summary,\n",
    "            \"sentiment\": self.sentiment,\n",
    "            \"entities\": self.entities,\n",
    "            \"highlighted_sentences\": self.highlighted_sentences,\n",
    "            \"questions\": self.questions\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, data: Dict):\n",
    "        doc = cls(data[\"path\"], data[\"content\"], data[\"title\"])\n",
    "        doc.short_summary = data[\"short_summary\"]\n",
    "        doc.summary = data[\"summary\"]\n",
    "        doc.sentiment = data[\"sentiment\"]\n",
    "        doc.entities = data[\"entities\"]\n",
    "        doc.highlighted_sentences = data[\"highlighted_sentences\"]\n",
    "        doc.questions = data[\"questions\"]\n",
    "        return doc\n",
    "    \n",
    "    def extract_highlighted_sentences(self) -> dict:\n",
    "        \n",
    "        highlighted_sentences = defaultdict(list)   \n",
    "\n",
    "        # Open PDF document\n",
    "        doc = pymupdf.open(self.path)\n",
    "        \n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc[page_num]\n",
    "            \n",
    "            # Get plain text content of the page\n",
    "            text = page.get_text()\n",
    "            \n",
    "            # Split text into sentences (basic splitting by period)\n",
    "            sentences = [s.strip() + '.' for s in text.split('.') if s.strip()]\n",
    "            \n",
    "            # Get highlights on the page\n",
    "            highlights = page.get_text_words()\n",
    "            annots = page.annots()\n",
    "            \n",
    "            if annots:\n",
    "                for annot in annots:\n",
    "                    if annot.type[0] == 8:  # Highlight annotation\n",
    "                        # Get highlight coordinates\n",
    "                        coords = annot.rect\n",
    "                        \n",
    "                        # Get color of highlight (normalize to RGB)\n",
    "                        color = annot.colors['stroke']\n",
    "                        if color:\n",
    "                            color_rgb = tuple(int(c * 255) for c in color)\n",
    "                        else:\n",
    "                            continue\n",
    "                        \n",
    "                        # Find words within highlight coordinates\n",
    "                        highlighted_words = []\n",
    "                        for word_info in highlights:\n",
    "                            word_rect = pymupdf.Rect(word_info[:4])\n",
    "                            if coords.intersects(word_rect):\n",
    "                                highlighted_words.append(word_info[4])\n",
    "                        \n",
    "                        if highlighted_words:\n",
    "                            # Find the sentence containing the highlighted words\n",
    "                            for sentence in sentences:\n",
    "                                if any(word.lower() in sentence.lower() for word in highlighted_words):\n",
    "                                    # Convert RGB tuple to hex for consistent key format\n",
    "                                    color_hex = '#{:02x}{:02x}{:02x}'.format(*color_rgb)\n",
    "                                    if sentence not in highlighted_sentences[color_hex]:\n",
    "                                        highlighted_sentences[color_hex].append(sentence)\n",
    "        \n",
    "        # Convert defaultdict to regular dict before returning it to the class\n",
    "        self.highlighted_sentences = dict(highlighted_sentences)\n",
    "\n",
    "    def get_pretty_highlights(self):\n",
    "        text = \"\"\n",
    "        \n",
    "        for color, sentences in self.highlighted_sentences.items():\n",
    "            print(f\"\\nHighlight Color: {color}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            text += \"\".join([f\"{i}. {sentence}\\n\" for i, sentence in enumerate(sentences, 1)])\n",
    "            \n",
    "        return text\n",
    "        \n",
    "        \n",
    "    def pretty_print_highlights(self):\n",
    "        print(self.get_pretty_highlights())                \n",
    "        \n",
    "                \n",
    "    def get_number_of_highlights(self) -> dict:\n",
    "        \"\"\"Function to get the number of highlights in the document\"\"\"\n",
    "        colors_count = dict()\n",
    "\n",
    "        for color, sentences in self.highlighted_sentences.items():\n",
    "            colors_count[color] = len(sentences)\n",
    "            \n",
    "        return colors_count\n",
    "    \n",
    "    def save2docx(self, file_path: str):\n",
    "        \"\"\"Exports document data, including word cloud, to a Word document.\"\"\"\n",
    "        document = Document()\n",
    "\n",
    "        # Add content to the Word document\n",
    "        document.add_heading(f\"Media Analysis - {self.title}\", level=1)\n",
    "\n",
    "        document.add_heading(\"Short Summary\", level=2)\n",
    "        document.add_paragraph(self.short_summary)\n",
    "\n",
    "        document.add_heading(\"Summary\", level=2)\n",
    "        document.add_paragraph(self.summary)\n",
    "        \n",
    "        for id, question in enumerate(self.questions):\n",
    "            document.add_heading(f\"Question {id+1}\", level=2)\n",
    "            document.add_paragraph(question)\n",
    "\n",
    "        document.add_heading(\"Sentiment\", level=2)\n",
    "        document.add_paragraph(f\"The sentiment is {self.sentiment}\")\n",
    "\n",
    "        document.add_heading(\"Entities\", level=2)\n",
    "        document.add_paragraph(\", \".join(self.entities))\n",
    "\n",
    "        document.add_heading(\"Highlights\", level=2)\n",
    "        document.add_paragraph(self.get_number_of_highlights())\n",
    "\n",
    "        # Add word clouds for each highlight color\n",
    "        try:\n",
    "            for color, _ in self.highlighted_sentences.items():\n",
    "                color_name = color.replace('#', '')\n",
    "                wordcloud_path = os.path.join(OUTPUT_FOLDER, self.title, f\"wordcloud_{color_name}.png\")\n",
    "                \n",
    "                # Check if the word cloud image exists\n",
    "                if os.path.exists(wordcloud_path):  \n",
    "                    document.add_heading(f\"Wordcloud for {color} Highlights\", level=2)\n",
    "                    document.add_picture(wordcloud_path, width=Inches(4.0))\n",
    "                    document.add_paragraph(\"Top 10 words:\")\n",
    "                    \n",
    "                    # Add top words and their frequencies\n",
    "                    for word, freq in self.wordcloud_data.get(color, []):\n",
    "                        document.add_paragraph(f\"- {word}: {freq}\")\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        # Save the document\n",
    "        document.save(file_path)\n",
    "                \n",
    "    def format2markdown(self) -> str:\n",
    "        \"\"\"Formats document data into a markdown string.\"\"\"\n",
    "\n",
    "        # Handle potential errors gracefully\n",
    "        try:\n",
    "            title = self.title\n",
    "        except AttributeError:\n",
    "            title = \"No Title\"\n",
    "            \n",
    "        try:\n",
    "            short_summary = self.short_summary\n",
    "        except AttributeError:\n",
    "            short_summary = \"No Short Summary\"\n",
    "            \n",
    "        try:\n",
    "            summary = self.summary\n",
    "        except AttributeError:\n",
    "            summary = \"No Summary\"\n",
    "        \n",
    "        try:\n",
    "            questions = [f\"Question {id+1}: {answer}\" for id, answer in enumerate(self.questions)]\n",
    "        except AttributeError:\n",
    "            questions = \"No Questions\"\n",
    "            \n",
    "        try:\n",
    "            sentiment = self.sentiment\n",
    "        except AttributeError:\n",
    "            sentiment = \"No Sentiment\"\n",
    "            \n",
    "        try:\n",
    "            entities = self.entities\n",
    "            entities_str = str(entities)\n",
    "        except AttributeError:\n",
    "            entities_str = \"No Entities\"\n",
    "            \n",
    "        try:\n",
    "            highlights = self.pretty_print_highlights()\n",
    "        except AttributeError:\n",
    "            highlights = \"No Highligts\"\n",
    "\n",
    "        # Construct the markdown string\n",
    "        markdown_output = f\"# Media Analysis - {title}\\n\\n\"\n",
    "        markdown_output += f\"# Short Summary\\n{short_summary}\\n\\n\"\n",
    "        markdown_output += f\"# Summary\\n{summary}\\n\\n\"\n",
    "        markdown_output += f\"# Questions\\n{questions}\\n\\n\"\n",
    "        markdown_output += f\"# Sentiment\\n{sentiment}\\n\\n\"\n",
    "        markdown_output += f\"# Entities\\n{entities_str}\\n\\n\"\n",
    "        markdown_output += f\"# Highlights\\n{highlights}\\n\\n\"\n",
    "            \n",
    "\n",
    "        # I deleted the word cloud part because it cannot be shown in markdown\n",
    "\n",
    "        return markdown_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PdfAnalyzer:\n",
    "    def __init__(self, entity_collection = \"all\", ollama_model: str = MODEL):\n",
    "        self.ollama_handler = OllamaHandler(ollama_model)\n",
    "        self.nlp = spacy.load(SPACY_MODEL) # Load spacy model\n",
    "        self.entitiy_collection = entity_collection if entity_collection in [\"all\", \"ollama\", \"spacy\"] else \"all\"\n",
    "        self.pdf_documents = []\n",
    "        \n",
    "    def extract_text_from_pdf(self, pdf_path: str) -> str:\n",
    "        \"\"\"Iterates over all pages in the document and stores the text in instance.\"\"\"\n",
    "\n",
    "        reader = PdfReader(pdf_path)\n",
    "        text = \"\"\n",
    "        num_pages = reader.pages\n",
    "        \n",
    "        for page_count, page in enumerate(num_pages):\n",
    "            text_current_page = page.extract_text()\n",
    "            print(f\"{datetime.now().strftime(\"%H:%M:%S\")}\\t Adding page {page_count}/{len(num_pages)} with {len(text_current_page)} characters\")\n",
    "            text += text_current_page\n",
    "        return text\n",
    "    \n",
    "    def extract_entities(self, text: str) -> List[str]:\n",
    "        \"\"\"Extracts entities from text using spacy PERSON and ORG labels.\"\"\"\n",
    "        \n",
    "        doc = self.nlp(text)\n",
    "        entities = [ent.text for ent in doc.ents if ent.label_ in [\"PERSON\", \"ORG\"]]\n",
    "        print(f\"{datetime.now().strftime(\"%H:%M:%S\")}\\t Found {len(entities)} in text\")\n",
    "        return list(set(entities))\n",
    "    \n",
    "    def process_pdf(self, pdf_path: str) -> PdfDocument:\n",
    "        \"\"\"Main function that processes a single PDF document with it's subfunctions. Prints status updates.\"\"\"\n",
    "        \n",
    "        content = self.extract_text_from_pdf(pdf_path)\n",
    "        content = self.clean_text(content, line_breaks=False)\n",
    "\n",
    "        title = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "        \n",
    "        # Initialize PdfDocument object\n",
    "        print(f\"{datetime.now().strftime(\"%H:%M:%S\")}\\t Create PDF document <{title[:20]}...> with content of length {len(content)}\")\n",
    "        pdf_doc = PdfDocument(pdf_path, content, title)\n",
    "        \n",
    "        print(f\"{datetime.now().strftime(\"%H:%M:%S\")}\\t Generating summary\")\n",
    "        short_summary_response = self.ollama_handler.generate_short_summary(content)\n",
    "        pdf_doc.short_summary = self.clean_text(short_summary_response)\n",
    "        \n",
    "        summary_response = self.ollama_handler.generate_summary(content)\n",
    "        pdf_doc.summary = self.clean_text(summary_response, soft_clean=True)\n",
    "\n",
    "        questions =  [\"How do the media (in our case = the sample we are analyzing) frame the public discussion about a given issue (in our case = ChatGPT)? Are there certain **metaphors** that keep cropping up?\",\n",
    "                      \"What **perspectives and aspects** of the topic are being widely covered, what aspects are being ignored?\",\n",
    "                      \"Which role does the Arabic World play in this article? How do they leverage AI? Answer with 'Not mentioned' if not applicable.\",\n",
    "                      \"What is the final message of the article? Keep short!\"]\n",
    "\n",
    "        print(f\"{datetime.now().strftime(\"%H:%M:%S\")}\\t Finding answer to {len(questions)} question{\"s\" if len(questions) > 1 else \"\"}\")\n",
    "        for question in questions:\n",
    "            answer = self.ollama_handler.answer_question(content, question)\n",
    "            pdf_doc.questions.append(self.clean_text(answer, soft_clean=True))\n",
    "                    \n",
    "        print(f\"{datetime.now().strftime(\"%H:%M:%S\")}\\t Analyzing sentiment\")\n",
    "        pdf_doc.sentiment = self.ollama_handler.analyze_sentiment(content)\n",
    "        \n",
    "        print(f\"{datetime.now().strftime(\"%H:%M:%S\")}\\t Extracting entities from text\")\n",
    "        \n",
    "        if self.entitiy_collection in [\"all\", \"spacy\"]:\n",
    "            # Get entities with spacy\n",
    "            entities_response = self.extract_entities(content)\n",
    "            pdf_doc.entities = [self.clean_text(ent) for ent in entities_response]\n",
    "            \n",
    "        if self.entitiy_collection in [\"all\", \"ollama\"]:\n",
    "            # Get entities with ollama\n",
    "            entities_response = self.ollama_handler.extract_entities(content)\n",
    "            pdf_doc.entities.extend([self.clean_text(ent) for ent in entities_response])\n",
    "        \n",
    "        print(f\"{datetime.now().strftime(\"%H:%M:%S\")}\\t Extracting text-highlights\")\n",
    "        pdf_doc.extract_highlighted_sentences()\n",
    "        \n",
    "        return pdf_doc\n",
    "    \n",
    "    def process_folder(self, PDF_FOLDER: str) -> List[PdfDocument]:\n",
    "        \"\"\"Iterates over all PDF files in the folder and processes them.\"\"\"\n",
    "        pdf_documents = []\n",
    "        for filename in os.listdir(PDF_FOLDER):\n",
    "            if filename.endswith(\".pdf\"):\n",
    "                pdf_path = os.path.join(PDF_FOLDER, filename)\n",
    "                pdf_doc = self.process_pdf(pdf_path)\n",
    "                pdf_documents.append(pdf_doc)\n",
    "        return pdf_documents\n",
    "    \n",
    "    def clean_text(self, text: str, soft_clean = False, line_breaks: bool = True) -> str:\n",
    "        \n",
    "        # replace ’ with '\n",
    "        #text = text.replace(\"’\", \"'\")\n",
    "        \n",
    "        # remove line breaks\n",
    "        text = text.replace(\"\\n\", \" \") if line_breaks and not soft_clean else text\n",
    "        \n",
    "        # remove non-ascii characters\n",
    "        text = text.encode(\"ascii\", \"ignore\").decode() if not soft_clean else text\n",
    "        \n",
    "        # remove all special characters except \"-\"       \n",
    "        text = re.sub(r\"[^a-zA-Z0-9.,* -]\", \" \", text) if not soft_clean else text\n",
    "        \n",
    "        # remove all double spaces\n",
    "        text = re.sub(r\"  \", \" \", text)\n",
    "        \n",
    "        # remove leading and trailing whitespaces\n",
    "        text = text.strip()\n",
    " \n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def save_documents(self, documents: List[PdfDocument], OUTPUT_FOLDER: str):\n",
    "        \"\"\"Saves the processed documents to a pickle file.\"\"\"\n",
    "        \n",
    "        path = os.path.join(OUTPUT_FOLDER, PROCESSED_DOC_FILENAME)\n",
    "        \n",
    "        if not os.path.exists(OUTPUT_FOLDER):\n",
    "            os.makedirs(OUTPUT_FOLDER)\n",
    "        \n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump([doc.to_dict() for doc in documents], f)\n",
    "    \n",
    "    def load_documents(self, input_path: str) -> List[PdfDocument]:\n",
    "        \"\"\"Loads the processed documents from a pickle file\"\"\"\n",
    "        with open(input_path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        return [PdfDocument.from_dict(doc_dict) for doc_dict in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_cloud(text_list, output_path = None, color_name=\"highlight\", width=800, height=400, background_color='white'):\n",
    "    # Load spacy model\n",
    "    nlp = spacy.load(SPACY_MODEL)\n",
    "    \n",
    "    # Combine all strings into one text\n",
    "    text = \" \".join(text_list)\n",
    "    text.replace(\"\\n\", \" \")\n",
    "    \n",
    "    # Process the text with spacy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Get non-stop words\n",
    "    words = [token.text for token in doc if not token.is_stop and token.is_alpha]\n",
    "    processed_text = \" \".join(words)\n",
    "    \n",
    "    # Create and generate a word cloud image\n",
    "    wordcloud = WordCloud(\n",
    "        width=width,\n",
    "        height=height,\n",
    "        background_color=background_color,\n",
    "        min_font_size=10,\n",
    "        max_font_size=150,\n",
    "        random_state=42\n",
    "    ).generate(processed_text)\n",
    "    \n",
    "    # Display the word cloud\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    \n",
    "    if output_path:\n",
    "        plt.savefig(f\"{output_path}/wordcloud_{color_name}.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    # Return most common words and their frequencies\n",
    "    word_freq = Counter(words).most_common(10)\n",
    "    return word_freq\n",
    "\n",
    "\n",
    "def process_highlights_with_wordcloud(doc: PdfDocument, output_folder) -> dict:\n",
    "    \"\"\" Iterates over each highlight color in the document and generates a word cloud for each color.\"\"\"\n",
    "    wordcloud_data = {}\n",
    "        \n",
    "    for highlight_color, sentences in doc.highlighted_sentences.items():\n",
    "        color_name = highlight_color.replace(\"#\", \"\")\n",
    "        path = os.path.join(output_folder, doc.title)\n",
    "        \n",
    "        if not os.path.exists(path):\n",
    "            print(f\"Creating folder: {path}\")\n",
    "            os.makedirs(path)\n",
    "            \n",
    "        word_frequencies = generate_word_cloud(\n",
    "            sentences,\n",
    "            output_path=os.path.join(output_folder, doc.title),\n",
    "            color_name=color_name\n",
    "        )\n",
    "        wordcloud_data[highlight_color] = word_frequencies\n",
    "    return wordcloud_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the PdfAnalyzer class\n",
    "analyzer = PdfAnalyzer(entity_collection = \"spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Process the documents in the folder where the PDFs are\n",
    "    documents = analyzer.process_folder(PDF_FOLDER)\n",
    "\n",
    "    # Save documents to the output folder\n",
    "    analyzer.save_documents(documents, OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load already analyzed documents\n",
    "loaded_documents = analyzer.load_documents(os.path.join(OUTPUT_FOLDER, PROCESSED_DOC_FILENAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a summary of every loaded file (Optional: with highlights)\n",
    "for doc in loaded_documents:\n",
    "    print(f\"Title: {doc.title}\")\n",
    "    print(f\"Short Summary: {doc.short_summary}\")\n",
    "    print(f\"Summary: {doc.summary}\")\n",
    "    #print(f\"Questions: {\"\\n\".join(doc.questions)}\")\n",
    "    print(f\"Sentiment: {doc.sentiment}\")\n",
    "    print(f\"Entities: {doc.entities}\")\n",
    "    \n",
    "    print(f\"Highlights: {doc.get_number_of_highlights()}\")\n",
    "    \n",
    "    if not doc.highlighted_sentences:\n",
    "        doc.extract_highlighted_sentences()\n",
    "        print(doc.pretty_print_highlights())\n",
    "    \n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word documents\n",
    "for doc in loaded_documents:\n",
    "    doc.wordcloud_data = process_highlights_with_wordcloud(doc, OUTPUT_FOLDER)\n",
    "    filename = f\"cai_media_analysis_{doc.filename}.docx\"\n",
    "    doc.save2docx(os.path.join(OUTPUT_FOLDER, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a markdown file for every document\n",
    "for doc in loaded_documents:\n",
    "    doc.wordcloud_data = process_highlights_with_wordcloud(doc, OUTPUT_FOLDER)\n",
    "    markdown_content = doc.format2markdown()\n",
    "    filename = f\"cai_media_analysis_{doc.filename}.md\"\n",
    "    with open(os.path.join(OUTPUT_FOLDER, filename), \"w\") as f:\n",
    "        f.write(markdown_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
