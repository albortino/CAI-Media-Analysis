{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from pypdf import PdfReader\n",
    "import matplotlib.pyplot as plt\n",
    "import ollama\n",
    "import spacy\n",
    "import dill as pickle\n",
    "import pymupdf\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "from docx import Document\n",
    "from docx.shared import Inches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model for ollama\n",
    "#MODEL = \"mistral\" \n",
    "#MODEL = \"llama3.1\"\n",
    "#MODEL = \"qwen:14b\"\n",
    "MODEL = \"phi4\"\n",
    "\n",
    "SPACY_MODEL = \"en_core_web_sm\"\n",
    "\n",
    "# Set folder paths\n",
    "PDF_FOLDER = \"Codings\"\n",
    "OUTPUT_FOLDER = os.path.join(\"Processed\", MODEL)\n",
    "PROCESSED_DOC_FILENAME = f\"{datetime.now().strftime(\"%y%m%d\")}-{MODEL}-processed_documents.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaHandler:\n",
    "    def __init__(self, model_name: str = MODEL):\n",
    "        self.model = model_name\n",
    "        \n",
    "    def generate_short_summary(self, text: str) -> str:\n",
    "        prompt = f\"Summarize the following text in exactly one (!) sentence withouth any further comments. Start your answer with 'The article is about...'. Article: {text}\"\n",
    "        response = ollama.generate(model=self.model, prompt=prompt)\n",
    "        original_response = response[\"response\"]\n",
    "        \n",
    "        return original_response.split(\".\")[0] + \".\"\n",
    "    \n",
    "    def generate_summary(self, text: str) -> str:\n",
    "        prompt = f\"Your are a research assistant and have been asked to summarize the following text in exactly 4 bullet points. It is extremely important that it is only four bullet points. Avoid adding any further comments or information that appears not in the text. Avoid mentioning that you are a research assistant and what your task is in your response. Here is the text you need to summarize: {text}\"\n",
    "        response = ollama.generate(model=self.model, prompt=prompt)\n",
    "        return response[\"response\"]\n",
    "    \n",
    "    def answer_question(self, text: str, question: str) -> str:\n",
    "        prompt = f\"Answer the following question based on the text, be concise and only mention topics that occured in the text: {text} Question: {question}\"\n",
    "        response = ollama.generate(model=self.model, prompt=prompt)\n",
    "        return response[\"response\"]\n",
    "    \n",
    "    def analyze_sentiment(self, text: str) -> float:\n",
    "        prompt = f\"Analyze the sentiment of the following text and respond with a single number between -5 (very negative towards AI, mentioned mainly risks) and +5 (very positive towards AI, mentioned mainly opportunities) without any comments: {text}\"\n",
    "        response = ollama.generate(model=self.model, prompt=prompt, system=\"You\")\n",
    "        try:\n",
    "            sentiment = float(re.search(r\"-?\\d+\\.?\\d*\", response[\"response\"]).group())\n",
    "            return max(min(sentiment, 5), -5)\n",
    "        except:\n",
    "            return 0.0\n",
    "        \n",
    "    def extract_entities(self, text: str) -> list:\n",
    "        prompt = f\"Extract the entities (Actors/spokespeople/institutions) from the following text. Separate them by a semicolon ; and only answer with the entities without any other comments: {text}\"\n",
    "        response = ollama.generate(model=self.model, prompt=prompt)\n",
    "        \n",
    "        response = response[\"response\"].replace(\"\\n\", \"\")\n",
    "                   \n",
    "        return response.split(\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PdfDocument:\n",
    "    def __init__(self, path: str, content: str, title: str):\n",
    "        self.path = path\n",
    "        self.content = content\n",
    "        self.title = title\n",
    "        self.filename = self.path.split(\"/\")[-1].split(\".\")[0]\n",
    "        self.short_summary = \"\"\n",
    "        self.summary = \"\"\n",
    "        self.sentiment = 0.0\n",
    "        self.entities = []\n",
    "        self.highlighted_sentences = defaultdict(list)     # Initialize dictionary to store sentences by highlight color\n",
    "        self.wordcloud_data = [] # List to store the information for the wordclouds\n",
    "        self.questions = []\n",
    "        \n",
    "        print(f\"{datetime.now().strftime(\"%H:%M:%S\")} Initialized PdfDocument: <{self.title}>\")\n",
    "        \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            \"path\": self.path,\n",
    "            \"content\": self.content,\n",
    "            \"title\": self.title,\n",
    "            \"short_summary\": self.short_summary,\n",
    "            \"summary\": self.summary,\n",
    "            \"sentiment\": self.sentiment,\n",
    "            \"entities\": self.entities,\n",
    "            \"highlighted_sentences\": self.highlighted_sentences,\n",
    "            \"questions\": self.questions\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, data: Dict):\n",
    "        doc = cls(data[\"path\"], data[\"content\"], data[\"title\"])\n",
    "        doc.short_summary = data[\"short_summary\"]\n",
    "        doc.summary = data[\"summary\"]\n",
    "        doc.sentiment = data[\"sentiment\"]\n",
    "        doc.entities = data[\"entities\"]\n",
    "        doc.highlighted_sentences = data[\"highlighted_sentences\"]\n",
    "        doc.questions = data[\"questions\"]\n",
    "        return doc\n",
    "    \n",
    "    def extract_highlighted_sentences(self) -> dict:\n",
    "        \n",
    "        highlighted_sentences = defaultdict(list)   \n",
    "\n",
    "        # Open PDF document\n",
    "        doc = pymupdf.open(self.path)\n",
    "        \n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc[page_num]\n",
    "            \n",
    "            # Get plain text content of the page\n",
    "            text = page.get_text()\n",
    "            \n",
    "            # Split text into sentences (basic splitting by period)\n",
    "            sentences = [s.strip() + '.' for s in text.split('.') if s.strip()]\n",
    "            \n",
    "            # Get highlights on the page\n",
    "            highlights = page.get_text_words()\n",
    "            annots = page.annots()\n",
    "            \n",
    "            if annots:\n",
    "                for annot in annots:\n",
    "                    if annot.type[0] == 8:  # Highlight annotation\n",
    "                        # Get highlight coordinates\n",
    "                        coords = annot.rect\n",
    "                        \n",
    "                        # Get color of highlight (normalize to RGB)\n",
    "                        color = annot.colors['stroke']\n",
    "                        if color:\n",
    "                            color_rgb = tuple(int(c * 255) for c in color)\n",
    "                        else:\n",
    "                            continue\n",
    "                        \n",
    "                        # Find words within highlight coordinates\n",
    "                        highlighted_words = []\n",
    "                        for word_info in highlights:\n",
    "                            word_rect = pymupdf.Rect(word_info[:4])\n",
    "                            if coords.intersects(word_rect):\n",
    "                                highlighted_words.append(word_info[4])\n",
    "                        \n",
    "                        if highlighted_words:\n",
    "                            # Find the sentence containing the highlighted words\n",
    "                            for sentence in sentences:\n",
    "                                if any(word.lower() in sentence.lower() for word in highlighted_words):\n",
    "                                    # Convert RGB tuple to hex for consistent key format\n",
    "                                    color_hex = '#{:02x}{:02x}{:02x}'.format(*color_rgb)\n",
    "                                    if sentence not in highlighted_sentences[color_hex]:\n",
    "                                        highlighted_sentences[color_hex].append(sentence)\n",
    "        \n",
    "        # Convert defaultdict to regular dict before returning it to the class\n",
    "        self.highlighted_sentences = dict(highlighted_sentences)\n",
    "\n",
    "    def get_pretty_highlights(self):\n",
    "        text = \"\"\n",
    "        \n",
    "        for color, sentences in self.highlighted_sentences.items():\n",
    "            print(f\"\\nHighlight Color: {color}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            text += \"\".join([f\"{i}. {sentence}\\n\" for i, sentence in enumerate(sentences, 1)])\n",
    "            \n",
    "        return text\n",
    "        \n",
    "        \n",
    "    def pretty_print_highlights(self):\n",
    "        print(self.get_pretty_highlights())                \n",
    "        \n",
    "                \n",
    "    def get_number_of_highlights(self) -> dict:\n",
    "        \"\"\"Function to get the number of highlights in the document\"\"\"\n",
    "        colors_count = dict()\n",
    "\n",
    "        for color, sentences in self.highlighted_sentences.items():\n",
    "            colors_count[color] = len(sentences)\n",
    "            \n",
    "        return colors_count\n",
    "    \n",
    "    def save2docx(self, file_path: str):\n",
    "        \"\"\"Exports document data, including word cloud, to a Word document.\"\"\"\n",
    "        document = Document()\n",
    "\n",
    "        # Add content to the Word document\n",
    "        document.add_heading(f\"Media Analysis - {self.title}\", level=1)\n",
    "\n",
    "        document.add_heading(\"Short Summary\", level=2)\n",
    "        document.add_paragraph(self.short_summary)\n",
    "\n",
    "        document.add_heading(\"Summary\", level=2)\n",
    "        document.add_paragraph(self.summary)\n",
    "        \n",
    "        for id, question in enumerate(self.questions):\n",
    "            document.add_heading(f\"Question {id+1}\", level=2)\n",
    "            document.add_paragraph(question)\n",
    "\n",
    "        document.add_heading(\"Sentiment\", level=2)\n",
    "        document.add_paragraph(f\"The sentiment is {self.sentiment}\")\n",
    "\n",
    "        document.add_heading(\"Entities\", level=2)\n",
    "        document.add_paragraph(\", \".join(self.entities))\n",
    "\n",
    "        document.add_heading(\"Highlights\", level=2)\n",
    "        document.add_paragraph(self.get_number_of_highlights())\n",
    "\n",
    "        # Add word clouds for each highlight color\n",
    "        try:\n",
    "            for color, _ in self.highlighted_sentences.items():\n",
    "                color_name = color.replace('#', '')\n",
    "                wordcloud_path = os.path.join(OUTPUT_FOLDER, self.title, f\"wordcloud_{color_name}.png\")\n",
    "                \n",
    "                # Check if the word cloud image exists\n",
    "                if os.path.exists(wordcloud_path):  \n",
    "                    document.add_heading(f\"Wordcloud for {color} Highlights\", level=2)\n",
    "                    document.add_picture(wordcloud_path, width=Inches(4.0))\n",
    "                    document.add_paragraph(\"Top 10 words:\")\n",
    "                    \n",
    "                    # Add top words and their frequencies\n",
    "                    for word, freq in self.wordcloud_data.get(color, []):\n",
    "                        document.add_paragraph(f\"- {word}: {freq}\")\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        # Save the document\n",
    "        document.save(file_path)\n",
    "                \n",
    "    def format2markdown(self) -> str:\n",
    "        \"\"\"Formats document data into a markdown string.\"\"\"\n",
    "\n",
    "        # Handle potential errors gracefully\n",
    "        try:\n",
    "            title = self.title\n",
    "        except AttributeError:\n",
    "            title = \"No Title\"\n",
    "            \n",
    "        try:\n",
    "            short_summary = self.short_summary\n",
    "        except AttributeError:\n",
    "            short_summary = \"No Short Summary\"\n",
    "            \n",
    "        try:\n",
    "            summary = self.summary\n",
    "        except AttributeError:\n",
    "            summary = \"No Summary\"\n",
    "        \n",
    "        try:\n",
    "            questions = [f\"Question {id+1}: {answer}\" for id, answer in enumerate(self.questions)]\n",
    "        except AttributeError:\n",
    "            questions = \"No Questions\"\n",
    "            \n",
    "        try:\n",
    "            sentiment = self.sentiment\n",
    "        except AttributeError:\n",
    "            sentiment = \"No Sentiment\"\n",
    "            \n",
    "        try:\n",
    "            entities = self.entities\n",
    "            entities_str = str(entities)\n",
    "        except AttributeError:\n",
    "            entities_str = \"No Entities\"\n",
    "            \n",
    "        try:\n",
    "            highlights = self.pretty_print_highlights()\n",
    "        except AttributeError:\n",
    "            highlights = \"No Highligts\"\n",
    "\n",
    "        # Construct the markdown string\n",
    "        markdown_output = f\"# Media Analysis - {title}\\n\\n\"\n",
    "        markdown_output += f\"# Short Summary\\n{short_summary}\\n\\n\"\n",
    "        markdown_output += f\"# Summary\\n{summary}\\n\\n\"\n",
    "        markdown_output += f\"# Questions\\n{questions}\\n\\n\"\n",
    "        markdown_output += f\"# Sentiment\\n{sentiment}\\n\\n\"\n",
    "        markdown_output += f\"# Entities\\n{entities_str}\\n\\n\"\n",
    "        markdown_output += f\"# Highlights\\n{highlights}\\n\\n\"\n",
    "            \n",
    "\n",
    "        # I deleted the word cloud part because it cannot be shown in markdown\n",
    "\n",
    "        return markdown_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PdfAnalyzer:\n",
    "    def __init__(self, entity_collection = \"all\", ollama_model: str = MODEL):\n",
    "        self.ollama_handler = OllamaHandler(ollama_model)\n",
    "        self.nlp = spacy.load(SPACY_MODEL) # Load spacy model\n",
    "        self.entitiy_collection = entity_collection if entity_collection in [\"all\", \"ollama\", \"spacy\"] else \"all\"\n",
    "        self.pdf_documents = []\n",
    "        \n",
    "    def extract_text_from_pdf(self, pdf_path: str) -> str:\n",
    "        \"\"\"Iterates over all pages in the document and stores the text in instance.\"\"\"\n",
    "\n",
    "        reader = PdfReader(pdf_path)\n",
    "        text = \"\"\n",
    "        num_pages = reader.pages\n",
    "        \n",
    "        for page_count, page in enumerate(num_pages):\n",
    "            text_current_page = page.extract_text()\n",
    "            print(f\"{datetime.now().strftime(\"%H:%M:%S\")}\\t Adding page {page_count}/{len(num_pages)} with {len(text_current_page)} characters\")\n",
    "            text += text_current_page\n",
    "        return text\n",
    "    \n",
    "    def extract_entities(self, text: str) -> List[str]:\n",
    "        \"\"\"Extracts entities from text using spacy PERSON and ORG labels.\"\"\"\n",
    "        \n",
    "        doc = self.nlp(text)\n",
    "        entities = [ent.text for ent in doc.ents if ent.label_ in [\"PERSON\", \"ORG\"]]\n",
    "        print(f\"{datetime.now().strftime(\"%H:%M:%S\")}\\t Found {len(entities)} in text\")\n",
    "        return list(set(entities))\n",
    "    \n",
    "    def process_pdf(self, pdf_path: str) -> PdfDocument:\n",
    "        \"\"\"Main function that processes a single PDF document with it's subfunctions. Prints status updates.\"\"\"\n",
    "        \n",
    "        content = self.extract_text_from_pdf(pdf_path)\n",
    "        content = self.clean_text(content, line_breaks=False)\n",
    "\n",
    "        title = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "        \n",
    "        # Initialize PdfDocument object\n",
    "        print(f\"{datetime.now().strftime(\"%H:%M:%S\")}\\t Create PDF document <{title[:20]}...> with content of length {len(content)}\")\n",
    "        pdf_doc = PdfDocument(pdf_path, content, title)\n",
    "        \n",
    "        print(f\"{datetime.now().strftime(\"%H:%M:%S\")}\\t Generating summary\")\n",
    "        short_summary_response = self.ollama_handler.generate_short_summary(content)\n",
    "        pdf_doc.short_summary = self.clean_text(short_summary_response)\n",
    "        \n",
    "        summary_response = self.ollama_handler.generate_summary(content)\n",
    "        pdf_doc.summary = self.clean_text(summary_response, soft_clean=True)\n",
    "\n",
    "        questions =  [\"How do the media (in our case = the sample we are analyzing) frame the public discussion about a given issue (in our case = ChatGPT)? Are there certain **metaphors** that keep cropping up?\",\n",
    "                      \"What **perspectives and aspects** of the topic are being widely covered, what aspects are being ignored?\",\n",
    "                      \"Which role does the Arabic World play in this article? How do they leverage AI? Answer with 'Not mentioned' if not applicable.\",\n",
    "                      \"What is the final message of the article? Keep short!\"]\n",
    "\n",
    "        print(f\"{datetime.now().strftime(\"%H:%M:%S\")}\\t Finding answer to {len(questions)} question{\"s\" if len(questions) > 1 else \"\"}\")\n",
    "        for question in questions:\n",
    "            answer = self.ollama_handler.answer_question(content, question)\n",
    "            pdf_doc.questions.append(self.clean_text(answer, soft_clean=True))\n",
    "                    \n",
    "        print(f\"{datetime.now().strftime(\"%H:%M:%S\")}\\t Analyzing sentiment\")\n",
    "        pdf_doc.sentiment = self.ollama_handler.analyze_sentiment(content)\n",
    "        \n",
    "        print(f\"{datetime.now().strftime(\"%H:%M:%S\")}\\t Extracting entities from text\")\n",
    "        \n",
    "        if self.entitiy_collection in [\"all\", \"spacy\"]:\n",
    "            # Get entities with spacy\n",
    "            entities_response = self.extract_entities(content)\n",
    "            pdf_doc.entities = [self.clean_text(ent) for ent in entities_response]\n",
    "            \n",
    "        if self.entitiy_collection in [\"all\", \"ollama\"]:\n",
    "            # Get entities with ollama\n",
    "            entities_response = self.ollama_handler.extract_entities(content)\n",
    "            pdf_doc.entities.extend([self.clean_text(ent) for ent in entities_response])\n",
    "        \n",
    "        print(f\"{datetime.now().strftime(\"%H:%M:%S\")}\\t Extracting text-highlights\")\n",
    "        pdf_doc.extract_highlighted_sentences()\n",
    "        \n",
    "        return pdf_doc\n",
    "    \n",
    "    def process_folder(self, PDF_FOLDER: str) -> List[PdfDocument]:\n",
    "        \"\"\"Iterates over all PDF files in the folder and processes them.\"\"\"\n",
    "        pdf_documents = []\n",
    "        for filename in os.listdir(PDF_FOLDER):\n",
    "            if filename.endswith(\".pdf\"):\n",
    "                pdf_path = os.path.join(PDF_FOLDER, filename)\n",
    "                pdf_doc = self.process_pdf(pdf_path)\n",
    "                pdf_documents.append(pdf_doc)\n",
    "        return pdf_documents\n",
    "    \n",
    "    def clean_text(self, text: str, soft_clean = False, line_breaks: bool = True) -> str:\n",
    "        \n",
    "        # replace ’ with '\n",
    "        #text = text.replace(\"’\", \"'\")\n",
    "        \n",
    "        # remove line breaks\n",
    "        text = text.replace(\"\\n\", \" \") if line_breaks and not soft_clean else text\n",
    "        \n",
    "        # remove non-ascii characters\n",
    "        text = text.encode(\"ascii\", \"ignore\").decode() if not soft_clean else text\n",
    "        \n",
    "        # remove all special characters except \"-\"       \n",
    "        text = re.sub(r\"[^a-zA-Z0-9.,* -]\", \" \", text) if not soft_clean else text\n",
    "        \n",
    "        # remove all double spaces\n",
    "        text = re.sub(r\"  \", \" \", text)\n",
    "        \n",
    "        # remove leading and trailing whitespaces\n",
    "        text = text.strip()\n",
    " \n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def save_documents(self, documents: List[PdfDocument], OUTPUT_FOLDER: str):\n",
    "        \"\"\"Saves the processed documents to a pickle file.\"\"\"\n",
    "        \n",
    "        path = os.path.join(OUTPUT_FOLDER, PROCESSED_DOC_FILENAME)\n",
    "        \n",
    "        if not os.path.exists(OUTPUT_FOLDER):\n",
    "            os.makedirs(OUTPUT_FOLDER)\n",
    "        \n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump([doc.to_dict() for doc in documents], f)\n",
    "    \n",
    "    def load_documents(self, input_path: str) -> List[PdfDocument]:\n",
    "        \"\"\"Loads the processed documents from a pickle file\"\"\"\n",
    "        with open(input_path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        return [PdfDocument.from_dict(doc_dict) for doc_dict in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_cloud(text_list, output_path = None, color_name=\"highlight\", width=800, height=400, background_color='white'):\n",
    "    # Load spacy model\n",
    "    nlp = spacy.load(SPACY_MODEL)\n",
    "    \n",
    "    # Combine all strings into one text\n",
    "    text = \" \".join(text_list)\n",
    "    text.replace(\"\\n\", \" \")\n",
    "    \n",
    "    # Process the text with spacy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Get non-stop words\n",
    "    words = [token.text for token in doc if not token.is_stop and token.is_alpha]\n",
    "    processed_text = \" \".join(words)\n",
    "    \n",
    "    # Create and generate a word cloud image\n",
    "    wordcloud = WordCloud(\n",
    "        width=width,\n",
    "        height=height,\n",
    "        background_color=background_color,\n",
    "        min_font_size=10,\n",
    "        max_font_size=150,\n",
    "        random_state=42\n",
    "    ).generate(processed_text)\n",
    "    \n",
    "    # Display the word cloud\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    \n",
    "    if output_path:\n",
    "        plt.savefig(f\"{output_path}/wordcloud_{color_name}.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    # Return most common words and their frequencies\n",
    "    word_freq = Counter(words).most_common(10)\n",
    "    return word_freq\n",
    "\n",
    "\n",
    "def process_highlights_with_wordcloud(doc: PdfDocument, output_folder) -> dict:\n",
    "    \"\"\" Iterates over each highlight color in the document and generates a word cloud for each color.\"\"\"\n",
    "    wordcloud_data = {}\n",
    "        \n",
    "    for highlight_color, sentences in doc.highlighted_sentences.items():\n",
    "        color_name = highlight_color.replace(\"#\", \"\")\n",
    "        path = os.path.join(output_folder, doc.title)\n",
    "        \n",
    "        if not os.path.exists(path):\n",
    "            print(f\"Creating folder: {path}\")\n",
    "            os.makedirs(path)\n",
    "            \n",
    "        word_frequencies = generate_word_cloud(\n",
    "            sentences,\n",
    "            output_path=os.path.join(output_folder, doc.title),\n",
    "            color_name=color_name\n",
    "        )\n",
    "        wordcloud_data[highlight_color] = word_frequencies\n",
    "    return wordcloud_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the PdfAnalyzer class\n",
    "analyzer = PdfAnalyzer(entity_collection = \"spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Process the documents in the folder where the PDFs are\n",
    "    documents = analyzer.process_folder(PDF_FOLDER)\n",
    "\n",
    "    # Save documents to the output folder\n",
    "    analyzer.save_documents(documents, OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:14:32 Initialized PdfDocument: <Will ChatGPT and AI have an impact on Saudi workforce productivity_ _ Arab News>\n",
      "01:14:32 Initialized PdfDocument: <AI is not smarter than humans _ Updated 08 April 2023>\n",
      "01:14:32 Initialized PdfDocument: <ChatGPT_ AI grows more powerful as we become more predictable _ Arab News>\n",
      "01:14:32 Initialized PdfDocument: <ChatGPT outperforms copywriters in STEP Conference’s outdoor adverts _ Updated 22 February 2023>\n",
      "01:14:32 Initialized PdfDocument: <Is the Arab world ready for the uncertain age of AI-powered web tools_Updated 09 March 2023>\n",
      "01:14:32 Initialized PdfDocument: <‘I am not here to take your job,’ ChatGPT tells Frankly Speaking host _Updated 20 March 2023>\n",
      "01:14:32 Initialized PdfDocument: <ChatGPT is the ‘Netscape moment’ for artificial intelligence’ _ Arab News>\n",
      "01:14:32 Initialized PdfDocument: <No need to demonize ChatGPT but AI regulation is a must _ Arab News>\n"
     ]
    }
   ],
   "source": [
    "# Load already analyzed documents\n",
    "loaded_documents = analyzer.load_documents(os.path.join(OUTPUT_FOLDER, PROCESSED_DOC_FILENAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Will ChatGPT and AI have an impact on Saudi workforce productivity_ _ Arab News\n",
      "Short Summary: The article is about how artificial intelligence technologies like ChatGPT could enhance Saudi Arabia s workforce productivity by boosting economic development, alleviating job loss fears through skill enhancement, increasing efficiency, and impacting various sectors such as healthcare, transportation, energy, finance, and retail, while requiring a strategic approach to integrate AI into operations effectively.\n",
      "Summary: - AI technologies like ChatGPT can enhance Saudi Arabia's workforce productivity by automating mundane tasks and enabling strategic focus. This fosters economic development through increased efficiency and innovation.\n",
      " \n",
      "- Concerns exist about job displacement due to AI; however, 50% of employees believe it boosts productivity while 51% see potential for better positions. Embracing AI involves cultivating a strong culture of learning and adaptation.\n",
      "\n",
      "- Raymond Khoury highlights AI's positive impacts across sectors such as healthcare, transportation, finance, and retail by optimizing operations and fostering innovation. However, the automation of tasks could threaten job stability unless employees upskill.\n",
      "\n",
      "- Implementing AI in Saudi companies requires a strategic approach that defines objectives, addresses bottlenecks, and selects appropriate technologies while fostering an environment conducive to learning and agility for successful integration.\n",
      "Sentiment: 3.0\n",
      "Entities: ['Arab News', 'Raymond Khoury', 'Arthur D. Little', 'Kaspersky', 'Arthur D. Little', 'Kasperskys', 'AI', 'Kingdoms', 'Embedding AI', 'Khourys', 'Saudi Arabias', 'Khoury']\n",
      "Highlights: {'#7cc867': 42, '#fb5b89': 18, '#f9cd59': 8, '#c885da': 8}\n",
      "--------------------------------------------------\n",
      "Title: AI is not smarter than humans _ Updated 08 April 2023\n",
      "Short Summary: The article is about the capabilities and limitations of artificial intelligence AI in enhancing personal and business tasks, emphasizing that while AI offers significant advancements and conveniences, it cannot replace human creativity, cultural understanding, or emotional connection, illustrated through examples like Apple s Siri and Microsoft s Tay.\n",
      "Summary: - AI tools, like ChatGPT, have become valuable for personal and business tasks by offering assistance akin to real-life recruiters or marketers, thanks to skilled engineers.\n",
      "- While AI can aid with grammar and brainstorming ideas, its reliability and human-like understanding of culture are still limited, as shown when Microsoft's chatbot Tay was misused shortly after release.\n",
      "- Although AI is praised for enhancing business operations and marketing, it cannot fully capture cultural nuances or replace the essential human connection in crafting meaningful experiences.\n",
      "- AI continues to evolve, facilitating communication across languages with tools like Google Translate, yet emphasizes that personalized messaging remains rooted in human input.\n",
      "Sentiment: 3.0\n",
      "Entities: ['Saint Marys University', 'Twitter', 'Microsoft', 'Tay', 'Google Translate', 'Saleh', 'Ali Al-Mustafa', 'Apple', 'SAP', 'Oracle']\n",
      "Highlights: {'#7cc867': 29, '#fb5b89': 29, '#f9cd59': 19}\n",
      "--------------------------------------------------\n",
      "Title: ChatGPT_ AI grows more powerful as we become more predictable _ Arab News\n",
      "Short Summary: The article is about the rapid adoption of ChatGPT, highlighting its impressive user growth as a sign of increasing human predictability due to internet use, while noting its current limitations in reasoning compared to human intelligence, alongside ethical concerns regarding AI development and emphasizing the need for thoughtful integration into society.\n",
      "Summary: - ChatGPT's adoption rate is unprecedented, reaching a million users within five days, highlighting a rapid embrace similar to other groundbreaking technologies.\n",
      " \n",
      "- Despite the excitement around AI-powered tools like ChatGPT, they lack the human mind's ability to reason and think deeply, relying instead on large datasets for content generation.\n",
      "\n",
      "- The increasing predictability of human behavior, influenced by decades of internet and smartphone use, enhances AI effectiveness but raises ethical concerns about reinforcing ideologies without critical reflection.\n",
      "\n",
      "- The future success of AI tools lies in collaboration with humans rather than complete replacement, urging companies and governments to consider gradual integration while reflecting on the implications for human intelligence.\n",
      "Sentiment: 2.0\n",
      "Entities: ['ChatGPT-4 https cdn.openai.com', 'Googles', 'Arab News', 'stu', 'Exponential View', 'OpenAI', 'TikTok', 'Twitter ibnezra https twitter.com', 'Noam Chomsky', 'Chomsky', 'The New York Times Whereas', 'iPhone', 'AI', 'ibnezra lang', 'Copyright Syndication Bureau', 'Joseph Dana', 'Gmail', 'JOSEPH DANA']\n",
      "Highlights: {'#fb5b89': 43, '#69aff0': 18, '#7cc867': 16, '#f9cd59': 20}\n",
      "--------------------------------------------------\n",
      "Title: ChatGPT outperforms copywriters in STEP Conference’s outdoor adverts _ Updated 22 February 2023\n",
      "Short Summary: The article is about how STEP Conferences utilized ChatGPT to create more satisfactory outdoor ad taglines than their agency or internal team could, leading them to plan continued use of the AI tool for various content creation tasks, while discussing broader implications of AI on creative industries and job roles.\n",
      "Summary: - ChatGPT was used for creating taglines in STEP Conferences' outdoor adverts after initial dissatisfaction with agency and internal team outputs.\n",
      "- The company plans to continue using AI tools like ChatGPT by acquiring a paid account for content creation, summarization, and explanation across the team.\n",
      "- While AI poses potential threats to jobs, experts argue it can create more opportunities, noting that many future job roles have yet to be invented.\n",
      "- STEP's founder sees AI as complementary to human talent, acknowledging its dual role in enhancing creativity while potentially replacing some tasks.\n",
      "Sentiment: 3.0\n",
      "Entities: ['Arab News', 'Art', 'STEP Conferences', 'Metas Open Pretrained Transformer', 'Googles Bard', 'Ray Dargham', 'STEP Conference', 'Dargham', 'Microsofts Bing', 'STEP', 'Dargham clari ed']\n",
      "Highlights: {'#7cc867': 12, '#f9cd59': 16, '#c885da': 12, '#fb5b89': 12}\n",
      "--------------------------------------------------\n",
      "Title: Is the Arab world ready for the uncertain age of AI-powered web tools_Updated 09 March 2023\n",
      "Short Summary: Certainly Here s a concise summary of the key points and themes from your provided text    Key Points 1.\n",
      "Summary: To transform the provided text into an outline format with subheadings, bullet points, and a separate section for data, we can organize the information as follows:\n",
      "\n",
      "---\n",
      "\n",
      "## Overview of Artificial Intelligence (AI) Developments\n",
      "\n",
      "### Introduction to AI's Impact on Various Sectors\n",
      "\n",
      "- **Content Creation and Assistance**\n",
      " - ChatGPT's role in generating text-based content across various applications.\n",
      " - Applications include social media posts, blogs, business plans, reports, emails, presentations, legal documents, medical summaries, and customer service responses.\n",
      "\n",
      "### Educational Implications of AI\n",
      "\n",
      "- Challenges posed by chatbots to educational institutions.\n",
      " - Adoption of paper-based tests to prevent AI cheating during exams.\n",
      " - Sanctions for students using ChatGPT at prestigious institutions like Sciences Po school in Paris.\n",
      " \n",
      "- Case Study: Minnesota University Law School\n",
      " - ChatGPT successfully passed law school exams, earning a C grade.\n",
      "\n",
      "### Technological Limitations and Capabilities\n",
      "\n",
      "- **ChatGPT's Abilities**\n",
      " - Generates text based on patterns seen during training.\n",
      " - Cannot replicate human creativity, emotion, or perspective.\n",
      "\n",
      "- Alternative AI Models Emerging alongside Large-Language Models (LLMs)\n",
      " - Reinforcement learning\n",
      " - Generative adversarial networks\n",
      " - Symbolic AI\n",
      "\n",
      "### Economic Impact and Investment Potential of AI\n",
      "\n",
      "- **Global Market Value and Growth**\n",
      " - Estimated global market value in 2022: $119.78 billion.\n",
      " - Expected contribution to the global economy by 2030: $15.7 trillion.\n",
      " - Projected growth over the next eight years: 13 times.\n",
      " - Number of people working in AI by 2024: 97 million.\n",
      "\n",
      "- Middle East's Share\n",
      " - Projected to accrue 2% of global benefits from AI, equivalent to $320 billion.\n",
      "\n",
      "### Trust and Future Outlook\n",
      "\n",
      "- **Balancing Optimism and Pessimism**\n",
      " - Dr. Scott Nowson emphasizes the importance of trust in AI's safe expansion.\n",
      " - Human intelligence remains crucial for leveraging AI effectively.\n",
      " - Long-term view: AI will not replace human capabilities anytime soon.\n",
      "\n",
      "---\n",
      "\n",
      "This outline captures the key points, data, and structure from the original text while organizing them into a clear format with subheadings and bullet points.\n",
      "Sentiment: 1.0\n",
      "Entities: ['Jenna Burrell', 'Bing', 'UAE', 'DeenSquare', 'AFP Founded', 'Generative Pre-Trained Transformer', 'LinkedIn', 'Dan Milmo', 'Alexa', 'PwC Middle Easts', 'Google', 'Ahmed Belhoul Al-Falasi', 'Reid Ho', 'OpenAI', 'Dubais Museum of the Future', 'AI', 'Edge', 'GPT', 'Intelligence', 'YouTube', 'James Webb', 'Arab News', 'Microsoft', 'LEAP', 'the World Economic Forum', 'Elon Musk', 'Nowson', 'AFP However', 'Sam Altman', 'the Sciences Po school', 'Noaman Sayed', 'Bard', 'Data Society', 'Scott Nowson', 'Omar Sultan Al-Olama', 'Peter Thiel', 'Googles', 'Minnesota University Law School', 'Guardian', 'ChatGPT', 'Burrell', 'New York Citys', 'Alex Hern', 'National Strategy for Data']\n",
      "Highlights: {'#7cc867': 48, '#fb5b89': 43, '#c885da': 19, '#f9cd59': 55}\n",
      "--------------------------------------------------\n",
      "Title: ‘I am not here to take your job,’ ChatGPT tells Frankly Speaking host _Updated 20 March 2023\n",
      "Short Summary: The text you provided offers an insightful overview of current discussions around AI, specifically focusing on ChatGPT s role and potential impact.\n",
      "Summary: The provided text explores the evolving landscape of artificial intelligence (AI), particularly focusing on OpenAI's ChatGPT and its integration into various technologies. Here's an overview:\n",
      "\n",
      "### Key Points\n",
      "\n",
      "1. **ChatGPT's Capabilities:**\n",
      "  - **Language Generation:** Known for generating natural, human-like text responses.\n",
      "  - **Image Understanding:** Recent updates allow it to process both text and image inputs, enhancing its application scope.\n",
      "\n",
      "2. **Industry Impact:**\n",
      "  - AI technologies like ChatGPT are becoming integral in industries such as healthcare, education, and communication by aiding tasks ranging from translation services to information access.\n",
      "  - Major tech companies, including Microsoft, Google (Bard), Amazon, Baidu, and Meta, are investing heavily in AI technologies to stay competitive.\n",
      "\n",
      "3. **Future Developments:**\n",
      "  - Focus areas include enhancing the naturalness of language models, improving context awareness, and personalizing responses through integration with other data sources.\n",
      "  - These advancements aim to meet user needs more effectively while promoting innovation within AI research.\n",
      "\n",
      "4. **Ethical Considerations:**\n",
      "  - The dual potential of AI as a beneficial or detrimental force underlines the importance of ethical usage.\n",
      "  - Issues such as privacy, bias, and job displacement are significant concerns that require attention from developers, policymakers, and users to ensure responsible use.\n",
      "\n",
      "5. **Vision for Humanity:**\n",
      "  - ChatGPT emphasizes the role of human decision-making in determining AI's impact on society.\n",
      "  - The ultimate goal is leveraging AI technology to enhance global wellbeing and foster a better future.\n",
      "\n",
      "This comprehensive perspective highlights both the opportunities and challenges presented by advanced AI technologies, underscoring the necessity of collaborative efforts towards ethical and beneficial implementations.\n",
      "Sentiment: -4.0\n",
      "Entities: ['Arab News', 'Google', 'Bing', 'OpenAI', 'Microsoft', 'Jensen', 'Amazon', 'ChatGPT', 'Meta', 'Baidu', 'Sam Altman', 'AI', 'Edge', 'Katie Jensen', 'Bard', 'Frankly Speaking']\n",
      "Highlights: {'#c885da': 29, '#f9cd59': 20, '#7cc867': 65, '#fb5b89': 50, '#69aff0': 7}\n",
      "--------------------------------------------------\n",
      "Title: ChatGPT is the ‘Netscape moment’ for artificial intelligence’ _ Arab News\n",
      "Short Summary: The article is about how ChatGPT represents a transformative moment for artificial intelligence similar to Netscape s impact on the World Wide Web, highlighting its capabilities, current limitations, potential future developments, and strategic importance for economic productivity and innovation, particularly in Saudi Arabia.\n",
      "Summary: - ChatGPT represents a significant moment for artificial intelligence similar to Netscape's impact on the World Wide Web, offering a rich conversational interface that has captured public interest.\n",
      "- It is built on Large Language Models (LLMs) trained using unsupervised learning on vast text corpuses, enabling tasks like question answering and content generation without explicit teaching of relationships between words.\n",
      "- Despite their capabilities, these models have limitations: they lack common sense, reasoning skills, and can generate plausible yet incorrect information due to understanding only language statistics.\n",
      "- As AI technology evolves, future systems will require augmentation with human-like attributes such as ethics and common sense to address challenges like misinformation, while foundation models offer new platforms for application development.\n",
      "Sentiment: 3.0\n",
      "Entities: ['exempli ed', 'Arab News', 'ANTHONY BUTLER', 'Anthony Butler', 'ChatGPT', 'the Saudi Data', 'Aramco', 'AI Authority', 'AI', 'Disclaimer Views', 'GPT', 'IBM']\n",
      "Highlights: {'#7cc867': 32, '#fb5b89': 13, '#c885da': 12}\n",
      "--------------------------------------------------\n",
      "Title: No need to demonize ChatGPT but AI regulation is a must _ Arab News\n",
      "Short Summary: The article is about the dual potential of ChatGPT to revolutionize various fields, including journalism, education, medicine, and literature, while also raising concerns over job displacement and ethical issues, emphasizing the need for AI regulation.\n",
      "Summary: - ChatGPT, developed by OpenAI, is a large language model capable of understanding and responding to natural language, impacting various industries including journalism in Saudi Arabia.\n",
      "- While seen as an opportunity for human advancement, concerns exist about AI's potential threat to jobs across sectors such as education and journalism; educators are adapting with new methods like oral exams to mitigate cheating through AI use.\n",
      "- In the medical field, ChatGPT has shown capabilities, even passing parts of a US Medical Licensing Examination without training on biomedical data, highlighting both its potential and limitations in practice.\n",
      "- The need for AI regulation is emphasized to balance its benefits against risks, ensuring it does not disproportionately favor technological advancements over human work.\n",
      "Sentiment: 3.0\n",
      "Entities: ['Arab News', 'Google', 'BBC', 'OpenAI', 'READ Learning', 'fed', 'The New York Times', 'Midjourney', 'AI', 'CNET', 'Springboard', 'Disclaimer Views', 'Generative Pretrained Transformer', 'The Washington Post', 'Post', 'Amal Mudallali']\n",
      "Highlights: {'#7cc867': 31, '#fb5b89': 50, '#c885da': 19, '#f9cd59': 33}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print a summary of every loaded file (Optional: with highlights)\n",
    "for doc in loaded_documents:\n",
    "    print(f\"Title: {doc.title}\")\n",
    "    print(f\"Short Summary: {doc.short_summary}\")\n",
    "    print(f\"Summary: {doc.summary}\")\n",
    "    #print(f\"Questions: {\"\\n\".join(doc.questions)}\")\n",
    "    print(f\"Sentiment: {doc.sentiment}\")\n",
    "    print(f\"Entities: {doc.entities}\")\n",
    "    \n",
    "    print(f\"Highlights: {doc.get_number_of_highlights()}\")\n",
    "    \n",
    "    if not doc.highlighted_sentences:\n",
    "        doc.extract_highlighted_sentences()\n",
    "        print(doc.pretty_print_highlights())\n",
    "    \n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in loaded_documents:\n",
    "    doc.wordcloud_data = process_highlights_with_wordcloud(doc, OUTPUT_FOLDER)\n",
    "    filename = f\"cai_media_analysis_{doc.filename}.docx\"\n",
    "    doc.save2docx(os.path.join(OUTPUT_FOLDER, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a markdown file for every document\n",
    "for doc in loaded_documents:\n",
    "    doc.wordcloud_data = process_highlights_with_wordcloud(doc, OUTPUT_FOLDER)\n",
    "    markdown_content = doc.format2markdown()\n",
    "    filename = f\"cai_media_analysis_{doc.filename}.md\"\n",
    "    with open(os.path.join(OUTPUT_FOLDER, filename), \"w\") as f:\n",
    "        f.write(markdown_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
